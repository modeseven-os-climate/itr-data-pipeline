{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41652f59-1798-4431-90dc-592dd4f64a7f",
   "metadata": {},
   "source": [
    "## Load RMI Utilities Transition Hub Data (from https://utilitytransitionhub.rmi.org/data-download/ for Data Vault Prototype)\n",
    "\n",
    "Copyright (C) 2023 OS-Climate\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "\n",
    "### We have local copies of the released datasets rooted in the S3_BUCKET : RMI/RMI-202*\n",
    "\n",
    "### The next step is to enrich with OS-C Factor metadata\n",
    "\n",
    "Contributed by Michael Tiemann (Github: MichaelTiemannOSC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92377eb7-1d1b-4662-ac08-99877153832b",
   "metadata": {},
   "source": [
    "Load Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d358e-36ae-4213-b0a3-ce1441137fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the AWS Account page, copy the export scripts from the appropriate role using the \"Command Line or Programmatic Access\" link\n",
    "# Paste the copied text into ~/credentials.env\n",
    "\n",
    "import os\n",
    "import io\n",
    "import pathlib\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "import osc_ingest_trino as osc\n",
    "\n",
    "import trino\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# Load environment variables from credentials.env\n",
    "osc.load_credentials_dotenv()\n",
    "\n",
    "iceberg_catalog = 'osc_datacommons_dev'\n",
    "iceberg_schema = 'rmi'\n",
    "rmi_table_prefix = ''\n",
    "\n",
    "engine = osc.attach_trino_engine(verbose=True, catalog=iceberg_catalog, schema=iceberg_schema)\n",
    "\n",
    "cwd = os.environ.get('PWD', '/opt/app-root/src') + '/rmi-utility-transition-hub-ingestion-pipeline'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5877739-a3c7-4f6c-ae92-b2288f511b41",
   "metadata": {},
   "source": [
    "Create an S3 resource for the bucket holding source data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355122da-ec89-4ac7-83aa-2774f18d20dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3_source = boto3.resource(\n",
    "    service_name=\"s3\",\n",
    "    endpoint_url=os.environ['S3_LANDING_ENDPOINT'],\n",
    "    aws_access_key_id=os.environ['S3_LANDING_ACCESS_KEY'],\n",
    "    aws_secret_access_key=os.environ['S3_LANDING_SECRET_KEY'],\n",
    ")\n",
    "source_bucket = s3_source.Bucket(os.environ['S3_LANDING_BUCKET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b50a54-3400-4920-bd78-acb4cd1795e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket must be configured with credentials for trino, and accessible to the hive catalog\n",
    "# You may need to use a different prefix here depending on how you name your credentials.env variables\n",
    "hive_bucket = osc.attach_s3_bucket('S3_HIVE')\n",
    "\n",
    "hive_catalog = 'osc_datacommons_hive_ingest'\n",
    "hive_schema = 'ingest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcccf42-6b82-48ac-b943-5438d1256f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_create = osc._do_sql(f\"\"\"\n",
    "create schema if not exists osc_datacommons_dev.{iceberg_schema}\n",
    "WITH (\n",
    "    location = 's3a://osc-datacommons-s3-bucket-dev02/data/{iceberg_schema}.db/'\n",
    ")\n",
    "\"\"\", engine, True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8b20c75d-8cb4-445e-8f52-0691c289fa0c",
   "metadata": {},
   "source": [
    "rmi_20210929_files = {}\n",
    "for file in source_bucket.objects.filter(Prefix='RMI/RMI-20210929'):\n",
    "    if file.key.endswith('csv'):\n",
    "        rmi_20210929_files[file.key] = file.last_modified.isoformat()\n",
    "# print(rmi_20210929_files)\n",
    "\n",
    "rmi_20211120_files = {}\n",
    "for file in source_bucket.objects.filter(Prefix='RMI/RMI-20211120'):\n",
    "    if file.key.endswith('csv'):\n",
    "        rmi_20211120_files[file.key] = file.last_modified.isoformat()\n",
    "# print(rmi_20211120_files)\n",
    "\n",
    "\n",
    "rmi_20220119_files = {}\n",
    "for file in source_bucket.objects.filter(Prefix='RMI/RMI-20220119'):\n",
    "    if file.key.endswith('csv'):\n",
    "        rmi_20220119_files[file.key] = file.last_modified.isoformat()\n",
    "print(rmi_20220119_files)\n",
    "\n",
    "rmi_20220307_files = {}\n",
    "for file in source_bucket.objects.filter(Prefix='RMI/RMI-20220307'):\n",
    "    if file.key.endswith('csv'):\n",
    "        rmi_20220307_files[file.key] = file.last_modified.isoformat()\n",
    "print(rmi_20220307_files)\n",
    "\n",
    "rmi_20230202_files = {}\n",
    "for file in source_bucket.objects.filter(Prefix='RMI/RMI-20230202'):\n",
    "    if file.key.endswith('csv'):\n",
    "        rmi_20230202_files[file.key] = file.last_modified.isoformat()\n",
    "print(rmi_20230202_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7eab8e-10be-4fa1-b9a2-05006bbaa9ac",
   "metadata": {},
   "source": [
    "Load RMI data file using pandas *read_csv* and appropriate dtype dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092d951-37d5-429b-955c-2ace5b1f3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Current (November 20, 2021) dataset sometimes represents YEAR as floating point number.  We will read as string and fix later.\n",
    "\n",
    "dtype_dict = {\n",
    "    # Table Name maps to dtypes (note that NaNs cannot encode to integers, so must do ex post facto fixes)\n",
    "    'assets_earnings_investments':\n",
    "        {'respondent_id':'int32',\n",
    "         'asset_value':'float64', 'earnings_value':'float64', 'investment_value':'float64'},\n",
    "    'customers_sales':\n",
    "        {'respondent_id':'int32',\n",
    "         'customers':'Int32',\n",
    "         'sales':'float64', 'revenues':'float64'},\n",
    "    'debt_equity_returns':\n",
    "        {'respondent_id':'int32',\n",
    "         'rate_base_actual':'float64', 'equity_actual':'float64', 'debt_actual':'float64', \n",
    "         'equity_ratio_actual':'float64', 'returns_actual':'float64', 'earnings_actual':'float64',\n",
    "         'interest_actual':'float64', 'fed_tax_expense_actual':'float64',\n",
    "         'pre_tax_net_income_actual':'float64', 'ROR_actual':'float64', 'ROE_actual':'float64',\n",
    "         'equity_ratio':'float64', 'ROR':'float64', 'ROE':'float64',\n",
    "         'interest_rate':'float64',\n",
    "         'effective_fed_tax_rate':'float64', 'equity_authorized':'float64', 'debt_authorized':'float64',\n",
    "         'returns_authorized':'float64', 'earnings_authorized':'float64', 'interest_authorized':'float64', \n",
    "         'interest_rate_authorized':'float64'},\n",
    "    'emissions_targets':\n",
    "        {'respondent_id':'Int32',\n",
    "         'CO2_historical':'float64', 'CO2_target':'float64', 'CO2_target_all_years':'float64', 'CO2_1point5C':'float64',\n",
    "         'generation_historical':'float64', 'generation_projected':'float64', 'generation_1point5C':'float64',\n",
    "         'CO2_intensity_historical':'float64', 'CO2_intensity_target':'float64', 'CO2_intensity_target_all_years':'float64', 'CO2_intensity_1point5C':'float64'},\n",
    "    'employees':\n",
    "        {'respondent_id':'int32',\n",
    "         'employees':'int32'},\n",
    "    'expenditure_bills_burden':\n",
    "        {'respondent_id':'int32',\n",
    "         'expenditure':'float64', 'bill':'float64', 'burden':'float64'},\n",
    "    'expenditure_bills_burden_detail': 'string',\n",
    "    'housing_units_income':\n",
    "        {'respondent_id':'int32',\n",
    "         'housing_units':'float64', 'income':'float64'},\n",
    "    'net_plant_balance':\n",
    "        {'respondent_id':'int32',\n",
    "         'original_cost':'float64', 'accum_depr':'float64', 'net_plant_balance':'float64',\n",
    "         'ARC':'float64', 'ARC_accum_depr':'float64', 'net_ARC':'float64'},\n",
    "    'operations_emissions_by_fuel':\n",
    "        {'respondent_id':'int32', 'plant_id_eia':'Int32',\n",
    "         'latitude':'float64', 'longitude':'float64',\n",
    "         'operating_month':'Int32', 'operating_year':'Int32',\n",
    "         'retirement_month':'Int32', 'retirement_year':'Int32',\n",
    "         'generation':'float64', 'fuel_consumption':'float64',\n",
    "         'emissions_c02':'float64', 'emissions_nox':'float64', 'emissions_sox':'float64'},\n",
    "    'operations_emissions_by_tech':\n",
    "        {'respondent_id':'int32', 'plant_id_eia':'Int32',\n",
    "         'latitude':'float64', 'longitude':'float64',\n",
    "         'capacity':'float64', 'year_end_capacity':'float64', 'generation':'float64', 'potential_generation':'float64',\n",
    "         'capacity_factor':'float64', 'fuel_consumption':'float64',\n",
    "         'emissions_co2':'float64', 'emissions_nox':'float64', 'emissions_sox':'float64'},\n",
    "    'revenue_by_tech':\n",
    "        {'respondent_id':'int32',\n",
    "         'revenue_total':'float64', 'revenue_residential':'float64' },\n",
    "    'state_policies': 'string',\n",
    "    'state_targets': 'string',\n",
    "    'utility_information':\n",
    "        {'respondent_id':'int32', 'utility_id_eia':'Int32',\n",
    "         'duplicate_utility_id_eia':'boolean' },\n",
    "    'utility_information_2023':\n",
    "        {'respondent_id':'int32', 'utility_id_ferc1':'Int32', 'utility_id_ferc1_dbf':'Int32',\n",
    "         'utility_id_eia':'Int32', 'fraction_owned_utility':'float64',\n",
    "         'duplicate_utility_id_eia':'boolean' },\n",
    "    'utility_state_map':\n",
    "        {'respondent_id':'int32',\n",
    "         'capacity_owned_in_state':'float64',\n",
    "         'capacity_operated_in_state':'float64',\n",
    "         'mwh_sales_in_state':'float64', },\n",
    "    'utility_state_map_2023':\n",
    "        {'utility_id_eia':'Int32',\n",
    "         'capacity_owned_in_state':'float64',\n",
    "         'capacity_operated_in_state':'float64',\n",
    "         'mwh_sales_in_state':'float64', },\n",
    "}\n",
    "\n",
    "fillna_dict = {\n",
    "    'assets_earnings_investments':\n",
    "        {'asset_value': 0, 'earnings_value': 0, 'investment_value': 0},\n",
    "    'customer_sales':\n",
    "        {'customers': 0, 'sales': 0, 'revenues': 0}\n",
    "}\n",
    "\n",
    "dropna_dict = {\n",
    "    'emissions_targets': {'respondent_id':'int32'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdb917b-ae80-4446-8539-552791ed8036",
   "metadata": {},
   "source": [
    "Create the actual metadata for the source.  In this case, it is *rmi_utility_transition_hub*\n",
    "\n",
    "We read and interpret the data dictionary that comes with the data sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6739837b-0e5d-4aa3-9794-8e2e70ab3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "if False:\n",
    "    rmi_20210929_b = io.BytesIO(source_bucket.Object('RMI/RMI-20210929.zip').get()['Body'].read())\n",
    "    rmi_20210929_zip = zipfile.ZipFile(rmi_20210929_b, mode='r')\n",
    "    del(rmi_20210929_b)\n",
    "    # display(zipfile.ZipFile(rmi_20210929_zip, mode='r').filelist)\n",
    "    rmi_dd = rmi_20210929_zip.read('data_download/RMI Utility Transition Hub Data Dictionary.xlsx')\n",
    "\n",
    "    # Read all the sheets.  rmi_excel['sheet_name'] gives a specific sheet\n",
    "    rmi_20210929_xls = pd.read_excel(rmi_dd, sheet_name=None, dtype=str)\n",
    "\n",
    "    rmi_20211120_b = io.BytesIO(source_bucket.Object('RMI/RMI-20211120.zip').get()['Body'].read())\n",
    "    rmi_20211120_zip = zipfile.ZipFile(rmi_20211120_b, mode='r')\n",
    "    del(rmi_20211120_b)\n",
    "    # display(zipfile.ZipFile(rmi_20211120_zip, mode='r').filelist)\n",
    "    rmi_dd = rmi_20211120_zip.read('data_dictionary.xlsx')\n",
    "\n",
    "    # Read all the sheets.  rmi_excel['sheet_name'] gives a specific sheet\n",
    "    rmi_20211120_xls = pd.read_excel(rmi_dd, sheet_name=None, dtype=str)\n",
    "\n",
    "# When updated, we'll get the data dictionary from 2023 zipfile.\n",
    "rmi_20220307_b = io.BytesIO(source_bucket.Object('RMI/RMI-20220307.zip').get()['Body'].read())\n",
    "rmi_20220307_zip = zipfile.ZipFile(rmi_20220307_b, mode='r')\n",
    "del(rmi_20220307_b)\n",
    "# display(zipfile.ZipFile(rmi_20220307_zip, mode='r').filelist)\n",
    "rmi_dd = rmi_20220307_zip.read('data_dictionary.xlsx')\n",
    "\n",
    "# Read all the sheets.  rmi_excel['sheet_name'] gives a specific sheet\n",
    "rmi_20220307_xls = pd.read_excel(rmi_dd, sheet_name=None, dtype=str)\n",
    "\n",
    "del(rmi_dd)\n",
    "\n",
    "rmi_20230202_b = io.BytesIO(source_bucket.Object('RMI/RMI-20230202.zip').get()['Body'].read())\n",
    "rmi_20230202_zip = zipfile.ZipFile(rmi_20230202_b, mode='r')\n",
    "del(rmi_20230202_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4064b65-3c45-4c42-a659-58861fbea62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "units_dict = {\n",
    "    '$': 'USD',\n",
    "    'number': '',\n",
    "}\n",
    "\n",
    "def drop_cols_by_index(df, index):\n",
    "    column_numbers = [x-1 for x in range(df.shape[1],0,-1)]  # reversed list of columns' integer indices\n",
    "    if type(index)==list:\n",
    "        index.sort()\n",
    "    else:\n",
    "        index = [ index ]\n",
    "    for i in index:\n",
    "        # removing small-to-large index values from large-to-small list means we take from the right, preserving order from zero origin\n",
    "        column_numbers.remove(i)\n",
    "    column_numbers.sort()\n",
    "    return df.iloc[:, column_numbers] # return remaining columns\n",
    "\n",
    "def generate_overview_meta(dd, release_date):\n",
    "    global overview_dd\n",
    "    print('Generating Overview...')\n",
    "    title = dd.iloc[0, 0]\n",
    "    general_overview = dd.iloc[0, 1]\n",
    "    scope = dd.iloc[1, 1]\n",
    "    limitations_to_scope = dd.iloc[2, 1]\n",
    "    # Drop non-table data captured above\n",
    "    dd = dd.iloc[4:]\n",
    "    dd.set_axis(['Name', 'Description'], axis=1,inplace=True)\n",
    "    meta_content = {'title':title, 'description':general_overview, 'version':f'Released {release_date}', 'uri':'https://utilitytransitionhub.rmi.org/data-download/',\n",
    "                    'copyright':'© 2021 RMI', 'license':'Creative Commons Attribution-Noncommercial 4.0 International Public License (CC BY-NC)',\n",
    "                    'contact':'utilitytransitionhub@rmi.org', 'abstract':'\\n'.join([scope, limitations_to_scope]), 'name':'rmi_utility_transition_hub' }\n",
    "    overview_dd = dd\n",
    "    return meta_content\n",
    "\n",
    "def get_meta_fields_from_dd(dd):\n",
    "    meta_fields = {k:{'description': v} for k, v in list(zip(dd['Data field'], dd['Definition']))}\n",
    "    for field, dim in list(zip(dd['Data field'], dd['Units'])):\n",
    "        if dim=='':\n",
    "            continue\n",
    "        if dim in units_dict:\n",
    "            meta_fields[field]['dimension'] = units_dict[dim]\n",
    "        else:\n",
    "            meta_fields[field]['dimension'] = dim\n",
    "    return meta_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea06cbe2-e9a7-4b68-b094-64a000e1e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_assets_meta(sheet, dd):\n",
    "    global overview_dd\n",
    "    description = overview_dd.loc[overview_dd['Name']==sheet, 'Description'].values[0]\n",
    "    meta_content = { 'tname':sheet, 'parent_schema':schemaname, 'description':description,}\n",
    "    if dd.iloc[-1]['Data field']=='Additional notes':\n",
    "        meta_content['Additional_notes'] = 'null'\n",
    "        dd = dd.drop(dd.index[-1:])\n",
    "    else:\n",
    "        meta_content['Additional_notes'] = dd.iloc[-1]['Data field']\n",
    "        dd = dd.drop(dd.index[-2:])\n",
    "\n",
    "    ### ??? What should we do with enumerations of asset and sub_asset types?\n",
    "\n",
    "    # Drop columns that exist only to hold enumeration values\n",
    "    dd = drop_cols_by_index(dd, [1,2])\n",
    "    # Asset & sub_asset are actually two fields\n",
    "    fixup_index = dd[dd['Data field'].eq('asset & sub_asset')].index\n",
    "    dd.loc[fixup_index, 'Data field'] = 'asset'\n",
    "    dd.loc[fixup_index, 'Definition'] = \"RMI's categorization of assets based on grouping of [steam,nuclear,hydro,renewables,other_fossil,transmission,distribution,other]\"\n",
    "    dd.loc[fixup_index+1, 'Data field'] = 'sub_asset'\n",
    "    dd.loc[fixup_index+1, 'Definition'] = \"RMI's categorization of sub_assets when asset=other based on grouping of [AROs,construction_work_in_progress,distribution_arc,electric_plant_held_for_future_use,electric_plant_leased_to_others,experimental_plant,general_plant,general_plant_arc,hydro_arc,intangible_plant,net_ADIT,net_regulatory_assets,net_working_capital,nuclear_arc,other_deferred_debits_and_credits,other_electric_plant,other_fossil_arc,other_noncurrent_liabilities,regional_transmission_and_market_operation,renewables_arc,steam_arc,transmission_arc]\"\n",
    "    dd = dd.dropna(subset=['Data field']).fillna(value='')\n",
    "\n",
    "    return get_meta_fields_from_dd(dd), meta_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd9e6dc-5f33-4ce4-9577-885a96d3849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bills_meta(sheet, dd):\n",
    "    global overview_dd\n",
    "    description = overview_dd.loc[overview_dd['Name']==sheet, 'Description'].values[0]\n",
    "    meta_content = { 'tname':sheet, 'parent_schema':schemaname, 'description':description,}\n",
    "    # Temporary fix for bad November 2020 data\n",
    "    if sheet=='revenue_by_tech':\n",
    "        dd = dd.rename(columns={' revenue_total ':'revenue_total', ' revenue_residential ':'revenue_residential'})\n",
    "    if dd.iloc[-1]['Data field']=='Additional notes':\n",
    "        meta_content['Additional_notes'] = 'null'\n",
    "        dd = dd.drop(dd.index[-1:])\n",
    "    else:\n",
    "        meta_content['Additional_notes'] = dd.iloc[-1]['Data field']\n",
    "        dd = dd.drop(dd.index[-2:])\n",
    "\n",
    "    ### ??? What should we do with code enumerations?\n",
    "\n",
    "    # Drop columns that exist only to hold enumeration values\n",
    "    dd = drop_cols_by_index(dd, 1)\n",
    "    # Strip out enumerations of codes\n",
    "    dd = dd.dropna(subset=['Data field']).fillna(value='')\n",
    "    return get_meta_fields_from_dd(dd), meta_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93df34a5-56e0-4a0e-81fc-46219b6adca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_emissions_meta(sheet, dd):\n",
    "    global overview_dd\n",
    "    global operations_emissions_fields, operations_emissions_content\n",
    "\n",
    "    description = overview_dd.loc[overview_dd['Name']==sheet, 'Description'].values[0]\n",
    "    meta_content = { 'tname':sheet, 'parent_schema':schemaname, 'description':description,}\n",
    "    if dd.iloc[-1]['Data field']=='Additional notes':\n",
    "        meta_content['Additional_notes'] = 'null'\n",
    "        dd = dd.drop(dd.index[-1:])\n",
    "    else:\n",
    "        meta_content['Additional_notes'] = dd.iloc[-1]['Data field']\n",
    "        dd = dd.drop(dd.index[-2:])\n",
    "\n",
    "    ### Should we put code enumerations into dbt/seeds data files?\n",
    "\n",
    "    # Drop columns that exist only to hold enumeration values\n",
    "    dd = drop_cols_by_index(dd, 1)\n",
    "    # Strip out enumerations of codes\n",
    "    dd = dd.dropna(subset=['Data field']).fillna(value='')\n",
    "    dd[dd.columns[0]] = dd[dd.columns[0]].str.lower()\n",
    "    meta_fields = get_meta_fields_from_dd(dd)\n",
    "    meta_fields['utility_id_eia'] = {'description':'Utility Code from EIA'}\n",
    "    del meta_fields['respondent_id']\n",
    "    meta_fields['utility_type_rmi'] = {'description':'Type of utility as classified by RMI' }\n",
    "    meta_fields['operational_status_code'] = meta_fields['status']\n",
    "    del meta_fields['status']\n",
    "    if 'fuel_type_code' in meta_fields:\n",
    "        meta_fields['energy_source_code'] = meta_fields['fuel_type_code']\n",
    "        del meta_fields['fuel_type_code']\n",
    "    # Change fieldnames of quantified fields after interpreting their dimensionality\n",
    "    meta_fields['net_generation'] = meta_fields['generation']\n",
    "    del meta_fields['generation']\n",
    "    meta_fields['fuel_consumed'] = meta_fields['fuel_consumption']\n",
    "    del meta_fields['fuel_consumption']\n",
    "\n",
    "    operations_emissions_fields = meta_fields\n",
    "    operations_emissions_content = meta_content\n",
    "    return meta_fields, meta_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39579169-ff6a-4985-b41d-b3e841e02e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This special case perfects the synthetic table we create ourselves (and completes the metadata describing it)\n",
    "\n",
    "def generate_other_generation_meta(dd):\n",
    "    global operations_emissions_fields\n",
    "    meta_content = { 'tname':'other_generation', 'parent_schema':schemaname,\n",
    "                     'description': 'EE & DR as well as Purchased Power, all of which count toward `avoided generation`',\n",
    "                     'Additional_notes': 'Table derived from operation_emissions_by_field by OS-Climate'}\n",
    "    meta_fields = operations_emissions_fields.copy()\n",
    "    for field in list(meta_fields.keys()):\n",
    "        if field not in dd.columns:\n",
    "            print(f\"Deleting obsolete field {field} from metadata\")\n",
    "            del meta_fields[field]\n",
    "    return meta_fields, meta_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d484e009-2254-49e2-9a58-05dd1b10022a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generic_meta(sheet, dd):\n",
    "    global overview_dd\n",
    "    \n",
    "    description = overview_dd.loc[overview_dd['Name']==sheet, 'Description'].values[0]\n",
    "    meta_content = { 'tname':sheet, 'parent_schema':schemaname, 'description':description, }\n",
    "    if dd.iloc[-1]['Data field']=='Additional notes':\n",
    "        meta_content['Additional_notes'] = 'null'\n",
    "        dd = dd.drop(dd.index[-1:])\n",
    "    else:\n",
    "        meta_content['Additional_notes'] = dd.iloc[-1]['Data field']\n",
    "        dd = dd.drop(dd.index[-2:])\n",
    "    return get_meta_fields_from_dd(dd), meta_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eec43e-3460-4821-82b7-3837676a0c17",
   "metadata": {},
   "source": [
    "The Data Dictionary is in an XLSX workbook\n",
    "The actual Data lives in seprate CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c44274f-789c-47f7-9f91-9569f7a0ea4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_generation_meta_fields = None\n",
    "other_generation_meta_content = None\n",
    "other_generation_df = None\n",
    "\n",
    "def generate_sheet_meta(wb, sheet, release_date, df):\n",
    "    \"\"\"\n",
    "    WB: The workbook defining the Data Dictionary\n",
    "    SHEET: The Data Dictionary for the table whose name is SHEET\n",
    "    RELEASE_DATE: The release date (version) of the Data Dictionary\n",
    "    DF: If not None, the dataframe for which we are constructing the metadata\n",
    "    \"\"\"\n",
    "\n",
    "    # For as-yet unexplained reasons, RMI renamed the file assets_earnings_investments.csv\n",
    "    # without updating the name of the sheet in the spreadsheet.  We switch to the intended sheetname here\n",
    "    if sheet == 'assets_earnings':\n",
    "        sheet = 'assets_earnings_investments'\n",
    "\n",
    "    if sheet[-5:] == '_2023':\n",
    "        dd = wb[sheet[:-5]]\n",
    "    else:\n",
    "        dd = wb[sheet]\n",
    "    # Remove empty column that appears in all of the spreadsheets\n",
    "    dd.columns = dd.iloc[0].values\n",
    "    dd = dd.iloc[1:, 1:]\n",
    "\n",
    "    if sheet=='Overview':\n",
    "        meta_content = generate_overview_meta(dd, release_date)\n",
    "        print('Metadata Overview')\n",
    "        print(meta_content)\n",
    "        return {}, meta_content\n",
    "    if sheet=='data_sources':\n",
    "        return {}, {}\n",
    "\n",
    "    # All other sheets have Data fields (some of which need downcasing)\n",
    "    dd['Data field'] = dd['Data field'].map(lambda x: x if not isinstance(x, str) or x=='Additional notes' else x.lower())\n",
    "    if sheet=='assets_earnings_investments':\n",
    "        return generate_assets_meta(sheet, dd)\n",
    "    if sheet=='emissions_targets':\n",
    "        meta_fields, meta_content = generate_generic_meta(sheet, dd)\n",
    "        meta_fields['target_scope'] = {'description':'Scope 1 (own generation) or Scope 3 (purchased generation)'}\n",
    "        return meta_fields, meta_content\n",
    "    if sheet=='utility_information':\n",
    "        meta_fields, meta_content = generate_generic_meta(sheet, dd)\n",
    "        meta_fields['entity_id'] = {'description':'XBRL-coded Utility ID from FERC'}\n",
    "        return meta_fields, meta_content\n",
    "    if sheet=='utility_information_2023':\n",
    "        meta_fields, meta_content = generate_generic_meta('utility_information', dd)\n",
    "        meta_fields = meta_fields.copy()\n",
    "        meta_fields['ticker'] = meta_fields['parent_ticker']\n",
    "        del meta_fields['parent_ticker']\n",
    "        meta_fields['isin'] = meta_fields['parent_isin']\n",
    "        del meta_fields['parent_isin']\n",
    "        for obsolete_key in ['respondent_id', 'first_report_year', 'last_report_year']:\n",
    "            del meta_fields[obsolete_key]\n",
    "        return meta_fields, meta_content\n",
    "    if sheet=='utility_state_map_2023':\n",
    "        meta_fields, meta_content = generate_generic_meta('utility_state_map', dd)\n",
    "        meta_fields = meta_fields.copy()\n",
    "        meta_fields['utility_id_eia'] = meta_fields['respondent_id']\n",
    "        del meta_fields['respondent_id']\n",
    "        return meta_fields, meta_content\n",
    "    if sheet in ['employees', 'expenditure_bills_burden', 'expenditure_bills_burden_detail', 'revenue_by_tech']:\n",
    "        # Both tables have the same essential shape\n",
    "        return generate_bills_meta(sheet, dd)\n",
    "    if sheet in ['operations_emissions_by_fuel', 'operations_emissions_by_tech']:\n",
    "        meta_fields, meta_content = generate_emissions_meta(sheet, dd)\n",
    "        return meta_fields, meta_content\n",
    "    if sheet=='other_generation':\n",
    "        return generate_other_generation_meta(dd)\n",
    "    # The sheet 'state_utility_policies' was renamed to 'state_policies' both in the dictionary and the filename\n",
    "    if sheet in ['state_policies', 'state_utility_policies']:\n",
    "        ### df = pd.read_csv(source_bucket.Object(f).get()['Body'],\n",
    "        ###                  dtype={'respondent_id':'int32'},parse_dates=['date_updated'],dayfirst=True)\n",
    "        return generate_generic_meta(sheet, dd)\n",
    "    return generate_generic_meta(sheet, dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54832ce5-0702-49e0-b952-7dae43edd290",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import datetime\n",
    "\n",
    "rmi_ingest_schemas = { # 'rmi_20210929': ('September 2021', rmi_20210929_xls, rmi_20210929_zip),\n",
    "                       # 'rmi_20211120': ('November 2021', rmi_20211120_xls, rmi_20211120_zip),\n",
    "                       # 'rmi_20220307: ('March 2022', rmi_20220307_xls, rmi_20220307_zip),\n",
    "                       'rmi': ('February 2023', rmi_20220307_xls, rmi_20230202_zip),}\n",
    "\n",
    "def first_valid_col_value(df, col):\n",
    "    return df.loc[df[col].first_valid_index(), col]\n",
    "\n",
    "dbt_dict = {}\n",
    "dbt_dict['models'] = dbt_models = {}\n",
    "# get rid of all the checkpoint files that dbt doesn't know how to ignore.\n",
    "# such brutal actions really test the limits of \"data reproducibility\": don't store model data you cannot re-create!\n",
    "shutil.rmtree(f\"{cwd}/dbt/rmi_transform/models\", ignore_errors=True)\n",
    "os.mkdir(f\"{cwd}/dbt/rmi_transform/models\", mode=0o755)\n",
    "\n",
    "custom_meta_content = custom_meta_fields = None\n",
    "\n",
    "# There is no datafile behind the data dictionary.  Run this to prime overview_dd, which all other metadata-finding depends upon\n",
    "for schemaname, (release_date, workbook, zipfile) in rmi_ingest_schemas.items():\n",
    "    # The dataframe for Non-Null data\n",
    "    df_nn = None\n",
    "    overview_dd = None\n",
    "    operations_emissions_content = None\n",
    "    operations_emissions_fields = None\n",
    "\n",
    "    overview_meta_fields, overview_meta_content = generate_sheet_meta(workbook, 'Overview', release_date, None)\n",
    "\n",
    "    for zipinfo in zipfile.infolist():\n",
    "        fname = zipinfo.filename\n",
    "        ftimestamp = datetime.datetime(*zipinfo.date_time)\n",
    "        if fname[-4:] != '.csv':\n",
    "            continue\n",
    "        if fname.startswith('__MACOSX/._'):\n",
    "            continue\n",
    "\n",
    "        tablename = fname.split('/')[-1].split('.')[0]\n",
    "        # For as-yet unexplained reasons, RMI renamed the file assets_earnings_investments.csv\n",
    "        # without updating the name of the sheet in the spreadsheet.  We switch to the intended name here\n",
    "        if tablename == 'assets_earnings':\n",
    "            tablename = 'assets_earnings_investments'\n",
    "        if tablename == 'expenditure_bills_burden_detail':\n",
    "            continue\n",
    "        print(f\"fname: '{fname}'; tablename = '{tablename}'\")\n",
    "        with zipfile.open(fname) as zf:\n",
    "            if tablename=='state_utility_policies':\n",
    "                df = pd.read_csv(zf, dtype={'respondent_id':'int32'},parse_dates=['date_updated'],dayfirst=True, engine='c')\n",
    "            elif tablename.startswith('utility'):\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], engine='c')\n",
    "                if release_date[-4:] <= '2021' and tablename=='utility_information':\n",
    "                    # Correct information for 'American Transmission Co LLC', which is owned by 'WEC Energy Group'\n",
    "                    df.loc[df.respondent_id==275, ['parent_name', 'parent_ticker', 'parent_ISIN', 'parent_LEI']] = df.loc[df.respondent_id==519, ['parent_name', 'parent_ticker', 'parent_ISIN', 'parent_LEI']].values\n",
    "                    df = df.rename(columns={'parent_ISIN':'parent_isin', 'parent_LEI':'parent_lei'})\n",
    "                if release_date[-4:] < '2023' and tablename=='utility_information':\n",
    "                    # Correct several LEI errors and omissions\n",
    "                    df.loc[df.parent_name=='American Electric Power Co., Inc.', 'parent_lei'] = '1B4S6S7G0TW5EE83BO58'\n",
    "                    df.loc[df.parent_name=='American States Water Co.', 'parent_isin'] = first_valid_col_value(df[df.parent_name=='American States Water Co.'], 'parent_isin')\n",
    "                    df.loc[df.parent_name=='American States Water Co.', 'parent_lei'] = '529900L26LIS2V8PWM23'\n",
    "                    # Berkshire Hathaway is 5493000C01ZX7D35SD85 but we're really dealing with the energy company,\n",
    "                    # which has the LEI 549300JD0S5IZJE9LY15.  But while the energy company has a detailed revenue breakdown,\n",
    "                    # it doesn't have a proper market cap.\n",
    "                    df.loc[df.parent_name=='Berkshire Hathaway, Inc.', 'parent_lei'] = '549300JD0S5IZJE9LY15'\n",
    "                    df.loc[df.parent_name=='Citizens Energy Corp.', 'parent_lei'] = '5493008ORX814MK1WM19'\n",
    "                    df.loc[df.parent_name=='FirstEnergy Corp.', 'parent_lei'] = '549300SVYJS666PQJH88'\n",
    "                    df.loc[df.parent_name=='LS Power', 'parent_lei'] = '549300Z88AAE0R1YHI77'\n",
    "                    # df.loc[df.parent_name=='National Grid PLC', 'parent_lei'] = 'MOM4570XTJ5YYX7JKH83'\n",
    "                    # df.loc[df.parent_name=='National Grid PLC', 'parent_isin'] = 'US6362744095'\n",
    "                    df.loc[df.parent_name=='NextEra Energy, Inc.', 'parent_lei'] = 'UMI46YPGBLUE4VGNNT48'\n",
    "                    # AEP owns OVEC\n",
    "                    df.loc[df.parent_name=='Ohio Valley Electric Corp.', 'parent_lei'] = '1B4S6S7G0TW5EE83BO58'\n",
    "                    df.loc[df.parent_name=='Ohio Valley Electric Corp.', 'parent_ticker'] = 'AEP'\n",
    "                    df.loc[df.parent_name=='Ohio Valley Electric Corp.', 'parent_isin'] = 'US0255371017'\n",
    "                    df.loc[df.parent_name=='PG&E Corp.', 'parent_lei'] = '8YQ2GSDWYZXO2EDN3511'\n",
    "                    df.loc[df.parent_name=='Sempra', 'parent_isin'] = first_valid_col_value(df[df.parent_name=='Sempra'], 'parent_isin')\n",
    "                    df.loc[df.parent_name=='Sempra', 'parent_lei'] = 'PBBKGKLRK5S5C0Y4T545'\n",
    "                    df.loc[df.parent_name=='Unitil Corp.', 'parent_lei'] = '549300EYGHO5EZE7RL80'\n",
    "                    df.loc[df.parent_name=='Verso Corp.', 'parent_lei'] = '549300FODXCTQ8DGT594'\n",
    "                    df.loc[df.parent_name=='Verso Corp.', 'parent_isin'] = 'US92531L2079'\n",
    "            elif tablename=='state_targets':\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], engine='c')\n",
    "                df.year.fillna('-1', inplace=True)\n",
    "                df.year = df.year.astype('string')\n",
    "                if release_date[-4:] < '2023':\n",
    "                    df.loc[df.year.isin(['Annual','2005/1990']), 'year'] = '-1'\n",
    "                df.year = pd.to_datetime(df.year.map(lambda x: x.split('.')[0]).astype('int32'), format='%Y', errors='coerce')\n",
    "            else:\n",
    "                df = pd.read_csv(zf, dtype=dtype_dict[tablename], thousands=',', engine='c')\n",
    "                if 'year' in df.columns:\n",
    "                    df.year.fillna('-1', inplace=True)\n",
    "                    df.year = df.year.astype('string')\n",
    "                    df.year = pd.to_datetime(df.year.map(lambda x: x.split('.')[0]).astype('int32'), format='%Y', errors='coerce')\n",
    "                if tablename=='revenue_by_tech':\n",
    "                    # The 2020 numbers come with extra spaces, comma separators, and sometimes parentheses instead of minus signs.\n",
    "                    def cleanup_2020_numbers(s):\n",
    "                        if type(s)==float:\n",
    "                            return s\n",
    "                        if s is None or s=='' or pd.isnull(s):\n",
    "                            return 'nan'\n",
    "                        s = s.strip().replace(',','')\n",
    "                        if s[0]=='(':\n",
    "                            s = s[1:-1]\n",
    "                        elif s=='#NAME?':\n",
    "                            s = 'nan'\n",
    "                        return s\n",
    "                    df.rename(columns={' revenue_total ':'revenue_total', ' revenue_residential ':'revenue_residential'}, inplace=True)\n",
    "                    if release_date[-4:] < '2022':\n",
    "                        df.revenue_total = df.revenue_total.map(cleanup_2020_numbers).astype('float64')\n",
    "                        df.revenue_residential = df.revenue_residential.map(cleanup_2020_numbers).astype('float64')\n",
    "                elif tablename=='debt_equity_returns':\n",
    "                    df.loc[~df.ROR_actual.map(np.isfinite), 'ROR_actual'] = 0.0\n",
    "                    df.loc[~df.ROE_actual.map(np.isfinite), 'ROE_actual'] = 0.0\n",
    "\n",
    "            df = df.convert_dtypes(infer_objects=False, convert_string=True, convert_integer=False, convert_boolean=False, convert_floating=False)\n",
    "            # df.info(verbose=True)\n",
    "\n",
    "        if tablename in dropna_dict:\n",
    "            df.dropna(subset=list(dropna_dict[tablename].keys()), inplace=True)\n",
    "        if tablename in fillna_dict:\n",
    "            df.fillna(value=fillna_dict[tablename], inplace=True)\n",
    "\n",
    "        custom_meta_fields, custom_meta_content = generate_sheet_meta(workbook, tablename, release_date, df)\n",
    "        if release_date[-4:] < '2023' and tablename in ['operations_emissions_by_fuel', 'operations_emissions_by_tech']:\n",
    "            # Both tables duplicate the 'Purchased Power' and 'EE & DR' data.\n",
    "            # We only need one copy, which we create as 'other_generation'\n",
    "            if df_nn is None:\n",
    "                df_anon = df.loc[df['plant_name_eia'].isna()].copy()\n",
    "                # Drop many NULL columns we don't need\n",
    "                df_anon.dropna(axis=1, how='all', inplace=True)\n",
    "                other_generation_df = df_anon\n",
    "                custom_gen_meta_fields, custom_gen_meta_content = generate_other_generation_meta(df_anon)\n",
    "\n",
    "                iceberg_table = f\"{rmi_table_prefix}other_generation\"\n",
    "                dbt_models[iceberg_table] = dbt_table = { 'description': custom_meta_content['description']}\n",
    "                if custom_meta_fields:\n",
    "                    dbt_table['columns'] = dbt_columns = (\n",
    "                        { name: {'description': custom_meta_fields[name]['description'] } for name in custom_meta_fields }\n",
    "                    )\n",
    "                    for name in custom_meta_fields:\n",
    "                        if 'tags' in custom_meta_fields[name]:\n",
    "                            dbt_columns[name]['tags'] = custom_meta_fields[name]['tags']\n",
    "\n",
    "                drop_table = osc._do_sql(f\"drop view if exists {iceberg_schema}.{iceberg_table}\", engine, verbose = False)\n",
    "                drop_table = osc._do_sql(f\"drop table if exists {iceberg_schema}.{iceberg_table}_source\", engine, verbose = False)\n",
    "                osc.fast_pandas_ingest_via_hive(\n",
    "                    df_anon,\n",
    "                    engine,\n",
    "                    iceberg_catalog, iceberg_schema, f\"{iceberg_table}_source\",\n",
    "                    hive_bucket, hive_catalog, hive_schema,\n",
    "                    partition_columns = ['year'],\n",
    "                    overwrite = True,\n",
    "                    typemap={\"datetime64[ns]\":\"timestamp(6)\", \"datetime64[ns, UTC]\":\"timestamp(6)\",\n",
    "                             # \"Int16\":\"integer\", \"int16\":\"integer\"\n",
    "                            },\n",
    "                    verbose = False\n",
    "                )\n",
    "                with open(f\"{cwd}/dbt/rmi_transform/models/{iceberg_table}.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    print(\"{{ config(materialized='view', view_security='invoker') }}\" + f\"\"\"\n",
    "with source_data as (\n",
    "    select {', '.join(df_anon.columns)}\n",
    "    from {iceberg_catalog}.{iceberg_schema}.{iceberg_table}_source\n",
    ")\n",
    "select * from source_data\n",
    "\n",
    "\"\"\", file=f)\n",
    "                df_nn = df.loc[~df['operating_month'].isna()]\n",
    "            df.dropna(subset=['plant_name_eia'], inplace=True)\n",
    "            # For some reason, data before 2010 is sometimes not filled in.\n",
    "            for index, row in df[df['operating_month'].isna()].iterrows():\n",
    "                # df_nn is only computed once, from either 'operations_emissions_by_fuel' or 'operations_emissions_by_tech'\n",
    "                df0 = df_nn.loc[(df_nn['respondent_id']==row['respondent_id']) & (df_nn['generator_id']==row['generator_id']), ['operating_month', 'operating_year']]\n",
    "                if len(df0)==0:\n",
    "                    # In this case we have no prior data to refer to\n",
    "                    continue\n",
    "                om, oy = df0.iloc[0]\n",
    "                df.loc[index, ('operating_month', 'operating_year')] = om, oy\n",
    "            # ICEBERG does not support integers less than 32 bits\n",
    "            # for colname in ['operating_month', 'operating_year', 'retirement_month', 'retirement_year']:\n",
    "            #     df[colname] = pd.to_numeric(df[colname],downcast='integer')\n",
    "            #     pass\n",
    "        elif tablename=='emissions_targets':\n",
    "            # Needed because respondent_id 191 data is duplicated (and 121 is kinda duplicated, too).\n",
    "            df.drop_duplicates(subset=None, keep='first', inplace=True, ignore_index=True)\n",
    "            df.loc[df['CO2_intensity_historical']==np.inf, ['CO2_historical', 'CO2_intensity_historical']] = [0, 0]\n",
    "            df['CO2_intensity_historical'] = df['CO2_intensity_historical'].astype('float64')\n",
    "\n",
    "            # Fix discrepencies between emissions_targets and the other files that define/reference parent_name\n",
    "            if release_date[-4:] < '2023':\n",
    "                df.loc[df.parent_name=='American States Water', 'parent_name'] = 'American States Water Co.'\n",
    "                df.loc[df.parent_name=='CMS Energy', 'parent_name'] = 'CMS Energy Corp.'\n",
    "                df.loc[df.parent_name=='Emera Inc.', 'parent_name'] = 'Versant Power'\n",
    "                df.loc[df.parent_name=='Fortis, Inc', 'parent_name'] = 'Fortis, Inc.'\n",
    "                df.loc[df.parent_name=='National Grid plc', 'parent_name'] = 'National Grid PLC'\n",
    "                df.loc[df.parent_name=='NorthWestern Corp.', 'parent_name'] = 'Northwestern Corp.'\n",
    "                df.loc[df.parent_name=='OG&E Energy', 'parent_name'] = 'OG&E Energy Corp.'\n",
    "                df.loc[df.parent_name=='PPL', 'parent_name'] = 'PPL Corp.'\n",
    "                df.loc[df.parent_name=='Sempra Energy', 'parent_name'] = 'Sempra'\n",
    "                df.loc[df.parent_name=='Verso', 'parent_name'] = 'Verso Corp.'\n",
    "        # if tablename in tidy_dict:\n",
    "        #     tidy_df = df.melt(id_vars=tidy_dict[tablename][0], value_vars=tidy_dict[tablename][2],\n",
    "        #                       var_name=tidy_dict[tablename][1][0], value_name=tidy_dict[tablename][1][1])\n",
    "        #     tidy_df[tidy_dict[tablename][1][0]] = tidy_df[tidy_dict[tablename][1][0]].apply(lambda x: x.split('_')[0])\n",
    "        #     tidy_df.dropna(subset=[tidy_dict[tablename][1][1]],inplace=True)\n",
    "\n",
    "        iceberg_table = f\"{rmi_table_prefix}{tablename}\"\n",
    "        if custom_meta_content:\n",
    "            dbt_models[iceberg_table] = dbt_table = { 'description': custom_meta_content['description']}\n",
    "            dbt_table['meta'] = custom_meta_content.copy()\n",
    "            del dbt_table['meta']['description']\n",
    "            if custom_meta_fields:\n",
    "                dbt_table['columns'] = dbt_columns = (\n",
    "                    { name: {'description': custom_meta_fields[name]['description'] } for name in custom_meta_fields }\n",
    "                )\n",
    "                for name in custom_meta_fields:\n",
    "                    if 'tags' in custom_meta_fields[name]:\n",
    "                        dbt_columns[name]['tags'] = custom_meta_fields[name]['tags']\n",
    "        elif custom_meta_fields:\n",
    "            raise VALUE_ERROR\n",
    "\n",
    "        if 'year' in df.columns:\n",
    "            partition = 'year'\n",
    "        elif 'first_report_year' in df.columns:\n",
    "            partition = 'first_report_year'\n",
    "        elif 'state' in df.columns:\n",
    "            partition = 'state'\n",
    "        elif 'utility_type_rmi' in df.columns:\n",
    "            partition = 'utility_type_rmi'\n",
    "        else:\n",
    "            raise KeyError\n",
    "        drop_table = osc._do_sql(f\"drop view if exists {iceberg_schema}.{iceberg_table}\", engine, verbose = False)\n",
    "        drop_table = osc._do_sql(f\"drop table if exists {iceberg_schema}.{iceberg_table}_source\", engine, verbose = False)\n",
    "        if len(df)>2000:\n",
    "            print(\"hive->iceberg path...\")\n",
    "            osc.fast_pandas_ingest_via_hive(\n",
    "                df,\n",
    "                engine,\n",
    "                iceberg_catalog, iceberg_schema, f\"{iceberg_table}_source\",\n",
    "                hive_bucket, hive_catalog, hive_schema,\n",
    "                partition_columns = [ partition ],\n",
    "                overwrite = True,\n",
    "                typemap={\"datetime64[ns]\":\"timestamp(6)\"},\n",
    "                verbose = False\n",
    "            )\n",
    "        else:\n",
    "            print(\"trino ingest path...\")\n",
    "            columnschema = osc.create_table_schema_pairs(df, typemap={\"datetime64[ns]\":\"timestamp(6)\"})\n",
    "            tabledef = f\"\"\"\n",
    "create table if not exists {iceberg_catalog}.{iceberg_schema}.{iceberg_table}_source(\n",
    "{columnschema}\n",
    ") with (\n",
    "    format = 'ORC',\n",
    "    partitioning = array['{partition}']\n",
    ")\n",
    "\"\"\"\n",
    "            qres = osc._do_sql(tabledef, engine, verbose=False)\n",
    "            df.to_sql(f\"{iceberg_table}_source\",\n",
    "                      con=engine, schema=iceberg_schema, if_exists='append',\n",
    "                      index=False,\n",
    "                      method=osc.TrinoBatchInsert(batch_size = 5000, verbose = False))\n",
    "        with open(f\"{cwd}/dbt/rmi_transform/models/{iceberg_table}.sql\", \"w\", encoding=\"utf-8\") as f:\n",
    "            print(\"{{ config(materialized='view', view_security='invoker') }}\" + f\"\"\"\n",
    "with source_data as (\n",
    "    select {', '.join(map(str.lower, df.columns))}\n",
    "    from {iceberg_catalog}.{iceberg_schema}.{iceberg_table}_source\n",
    ")\n",
    "select * from source_data\n",
    "\n",
    "\"\"\", file=f)\n",
    "    zipfile.close()\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3096b05a-b171-4cef-8a47-12fb957867b3",
   "metadata": {},
   "source": [
    "Iterate through tablenames until we get to *Additional Information*, storing all the names and descriptions of the tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b07bf75-c64a-4059-936d-4bd2253fa0d9",
   "metadata": {},
   "source": [
    "We've already collected the field names, types, dimensions, etc., of each table into our dataframes.\n",
    "But there's additional metadata we can collect from the data dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fde1a9-b84d-4d16-a89c-38d939ab71a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load metadata following an ingestion process into trino metadata store\n",
    "\n",
    "### The schema is *metastore*, and the table names are *meta_schema*, *meta_table*, *meta_field*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a250c477-b73a-4c99-b889-d48e7b756df9",
   "metadata": {},
   "source": [
    "metastore = 'metastore'\n",
    "\n",
    "# Create a metadata schema with tables for the three layers of metadata: schema, table, and field.\n",
    "\n",
    "meta_schema = 'meta_schema'\n",
    "meta_table = 'meta_table'\n",
    "meta_field = 'meta_field'\n",
    "\n",
    "# These metadata tables are local to this ingestion process.\n",
    "# We will insert/merge with master metadata tables later\n",
    "\n",
    "metadata_to_df = {\n",
    "    # For each data source there is a single entry in the _schema_table\n",
    "    meta_schema: pd.DataFrame(data=[], columns=[]),\n",
    "    # For each data source there are one or more tables in the _tables_table\n",
    "    meta_table: pd.DataFrame(data=[],\n",
    "                    columns=['tname', 'parent_schema', 'source', 'processing_pipeline']),\n",
    "    # For each table there are one or more fields in the fields_table\n",
    "    meta_field: pd.DataFrame(data=[],\n",
    "                    columns=['fname', 'parent_table', 'type', 'dimension', 'description'])\n",
    "}\n",
    "\n",
    "cur.execute(f'create schema if not exists {metastore}')\n",
    "cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fbfabd-9dbc-4873-bd89-cc405a436283",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_yml = open(f\"{cwd}/dbt/rmi_transform/rmi_base_schema.yml\", \"w\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a35c7-85ad-45fe-9b68-3c371a51a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"version: 2\", file=dbt_yml)\n",
    "\n",
    "indent = 0\n",
    "print(\"\\nmodels:\", file=dbt_yml)\n",
    "indent = indent + 2\n",
    "for name in dbt_dict['models']:\n",
    "    model = dbt_dict['models'][name]\n",
    "    print(f\"{' '*indent}- name: {name}\", file=dbt_yml)\n",
    "    indent = indent + 2\n",
    "    print(f\"{' '*indent}description: {json.dumps(model['description'])}\", file=dbt_yml)\n",
    "    meta = model.get('meta', None)\n",
    "    if meta:\n",
    "        print(f\"{' '*indent}config:\", file=dbt_yml)\n",
    "        indent = indent + 2\n",
    "        print(f\"{' '*indent}meta: {{\", file=dbt_yml)\n",
    "        indent = indent + 2\n",
    "        for meta_key in meta:\n",
    "            print(f\"{' '*indent}{meta_key}: {json.dumps(meta[meta_key])},\", file=dbt_yml)\n",
    "        indent = indent - 4\n",
    "        print(f\"{' '*indent}  }}\", file=dbt_yml)\n",
    "    print(f\"\\n{' '*indent}columns:\", file=dbt_yml)\n",
    "    indent = indent + 2\n",
    "    columns = model['columns']\n",
    "    for col in columns:\n",
    "        print(f\"{' '*indent}- name: {col}\", file=dbt_yml)\n",
    "        indent = indent + 2\n",
    "        for col_meta in columns[col]:\n",
    "            print(f\"{' '*indent}{col_meta}: {json.dumps(columns[col][col_meta])}\", file=dbt_yml)\n",
    "        indent = indent - 2\n",
    "    print(\"\", file=dbt_yml) # newline comes for free...\n",
    "    indent = indent - 4\n",
    "indent = indent - 2\n",
    "assert(indent==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98dcba2-a8b5-46c6-8d2f-1391f57794dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbt_yml.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9e8a2-3159-41af-96b4-0775856b0c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
